{
 "metadata": {
  "name": "",
  "signature": "sha256:4cdbb12226ae9d3c48245cfc293f3b0a796bb4d30747ba74baaf5a35e86f2f81"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayesian inversion for tectonic stresses\n",
      "## Step 1: Using coseismic slip rake as a constraint"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we estimate tectonic stress by adding $p(T)$ samples to topographic stress and lithostatic stress on the fault, and selecting $p(T)$ that yeild maximum shear stresses\n",
      "aligned with coseismic slip."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sys\n",
      "import halfspace.projections as hsp\n",
      "import time\n",
      "\n",
      "sys.path.append('../aux_scripts/')\n",
      "import stress_comps_vectorized as scv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "out_name = '../results/baloch_rake_posteriors.csv'\n",
      "fault_file = '../test_data/baloch_fault_model.csv'\n",
      "\n",
      "fault_df = pd.read_csv(fault_file, index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(69)\n",
      "\n",
      "# some initial constants\n",
      "n_trials = 1e5 # 1e4 for testing\n",
      "n_points = len(fault_df.index)\n",
      "rho = 2700\n",
      "g = 9.81"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Make priors\n",
      "\n",
      "These are the priors for tectonic stress (txx, tyy, txy).\n",
      "\n",
      "\n",
      "These are functions of lithostatic pressure (rho g depth)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s1s = np.random.uniform(0,2.5, n_trials)\n",
      "s3s = np.random.uniform(0, 1, n_trials) * s1s\n",
      "thetas = np.random.uniform(0, 2 * np.pi, n_trials)\n",
      "\n",
      "xxs = scv.xx_stress_from_s1_s3_theta(s1s, s3s, thetas)\n",
      "yys = scv.yy_stress_from_s1_s3_theta(s1s, s3s, thetas)\n",
      "xys = scv.xy_stress_from_s1_s3_theta(s1s, s3s, thetas)\n",
      "\n",
      "del s1s, s3s, thetas  # save some RAM (important for large n_trials)\n",
      "\n",
      "xxs = xxs.reshape([n_trials, 1])\n",
      "yys = yys.reshape([n_trials, 1])\n",
      "xys = xys.reshape([n_trials, 1])\n",
      "\n",
      "t_priors = np.concatenate((xxs, yys, xys), axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Set up 'search' DataFrame\n",
      "\n",
      "'Search' is old terminology from initial work on gridsearch.  I suppose we are still searching parameter space, though..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Columns for search dataframe\n",
      "search_df_cols = ['iter','txx', 'tyy', 'txy', 'pt_index', 'depth', 'strike',\n",
      "                  'dip', 'slip_m', 'slip_rake']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we make a big index for the search DataFrame, which is heirarchical, with indices for both the iteration number and the fault points.\n",
      "\n",
      "This process can take a long time and use a lot of RAM with large `n_trials`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make index list\n",
      "iter_range = np.arange(n_trials, dtype='float')\n",
      "pt_range = np.arange(n_points, dtype='float')\n",
      "\n",
      "index_list = [[iter_range[i],t_priors[i,0],t_priors[i,1],t_priors[i,2], pi]\n",
      "             for i in iter_range for pi in pt_range]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now make a NumPy array out of it and delete the list\n",
      "index_array = np.array(index_list)\n",
      "del index_list\n",
      "\n",
      "iter_index = np.int_(index_array[:,0].copy() )\n",
      "pt_index = np.int_(index_array[:,4].copy() )\n",
      "prior_array = index_array[:,1:4]\n",
      "del index_array #save some space"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### now make dataframe"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_df=pd.DataFrame(index=np.arange(len(iter_index)), \n",
      "                       columns=search_df_cols, dtype=float)\n",
      "\n",
      "search_df['iter'] = iter_index\n",
      "search_df[search_df_cols[1:4] ] =prior_array\n",
      "search_df['pt_index'] = pt_index\n",
      "\n",
      "search_df['mxx'] = 0.\n",
      "search_df['myy'] = 0.\n",
      "search_df['mxy'] = 0.\n",
      "search_df['mxz'] = 0.\n",
      "search_df['myz'] = 0.\n",
      "search_df['mzz'] = 0.\n",
      "\n",
      "# This is a list of the columns in the search_df that we are going to\n",
      "# fill with values from the fault slip dataframe.\n",
      "df_fill_cols = ['depth', 'strike', 'dip', 'slip_m', 'slip_rake',\n",
      "                'mxx', 'myy', 'mxy', 'mzz', 'mxz', 'myz']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This (`df_copy_cols`) might require a little configuration if the names in the fault dataframe are different than those in this example.  But there needs to be a 1:1 correspondance between the column name and order in `df_fill_cols` and `df_copy_cols`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_copy_cols = ['z', 'strike','dip','slip_m', 'rake',\n",
      "                'xx_stress', 'yy_stress', 'xy_stress', 'zz_stress',\n",
      "                'xz_stress', 'yz_stress']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are going to copy the relevant columns from `fault_df`, make `n_trials` copies, and fill them into `search_df`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_col_array = fault_df[df_copy_cols].values    # copy columns\n",
      "df_reps = np.tile(df_col_array, [n_trials, 1])  # make sequential copies\n",
      "search_df[df_fill_cols] = df_reps               # fill df\n",
      "search_df.depth *= 1000                         # make depth into m (orig km)\n",
      "\n",
      "del df_reps # save some RAM\n",
      "\n",
      "# convert from MPa to Pa\n",
      "search_df[['mxx', 'myy', 'mxy', 'mzz', 'mxz', 'myz']] *= 1e6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Calculate fault stresses from priors\n",
      "\n",
      "We include fixed lithostatic and topographic stresses (mxx, etc.), and tectonic stresses (txx, etc.) sampled from the priors."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_df['tau_s'] = scv.strike_shear(strike = search_df.strike, \n",
      "                                      dip=search_df.dip, rho=rho, g=g,\n",
      "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
      "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
      "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
      "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
      "                                      txy=search_df.txy, depth=search_df.depth)\n",
      "\n",
      "search_df['tau_d'] = scv.dip_shear(strike = search_df.strike, \n",
      "                                   dip=search_df.dip, rho=rho, g=g,\n",
      "                                   mxx=search_df.mxx, myy=search_df.myy,\n",
      "                                   mxy=search_df.mxy, mzz=search_df.mzz,\n",
      "                                   mxz=search_df.mxz, myz=search_df.myz,\n",
      "                                   txx=search_df.txx, tyy=search_df.tyy,\n",
      "                                   txy=search_df.txy, depth=search_df.depth)\n",
      "\n",
      "search_df['tau_rake'] = hsp.get_rake_from_shear_components(strike_shear=\n",
      "                                                               search_df.tau_s,\n",
      "                                                           dip_shear=\n",
      "                                                               search_df.tau_d)\n",
      "\n",
      "search_df['rake_misfit_rad'] = np.radians(scv.angle_difference(\n",
      "                                                          search_df.slip_rake,\n",
      "                                                          search_df.tau_rake,\n",
      "                                                          return_abs=True) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Calculate likelihood"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate weighted rake misfit on each point"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum_slip = fault_df.slip_m.sum()\n",
      "\n",
      "# we will consider the 1-sigma-ish error on rake to be pi/9\n",
      "rake_err = np.pi/9\n",
      "kappa = 8.529 #calculated so that 68.2% of von mises is within pi/9 of mean\n",
      "\n",
      "search_df['weighted_diff'] = search_df.rake_misfit_rad * search_df.slip_m"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we will break the `search_df` into groups, one for each iteration/prior sample."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iters = search_df.groupby('iter')\n",
      "\n",
      "del search_df # we don't need the whole thing anymore."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "calculate likelihood for each group"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fish_l1 = iters.weighted_diff.sum() / sum_slip  # L1 likelihood w/ Fisher distribution\n",
      "\n",
      "fish_like = np.exp(kappa * np.cos(fish_l1) )    # \n",
      "fish_like /= fish_like.max()                    # Normalize by best fit"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Sample posterior proportional to likelihood"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rand_filter = np.random.random(n_trials)\n",
      "\n",
      "fishtrap = fish_like[fish_like >= rand_filter] # I'm so punny\n",
      "\n",
      "print('Retained {} out of {} samples'.format(len(fishtrap), int(n_trials)) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Get the posterior values back"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iters_txx = iters.txx.mean()  # same as xxs, yys, etc. but now w/ iter index\n",
      "iters_tyy = iters.tyy.mean()\n",
      "iters_txy = iters.txy.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See which priors were retained"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "txx_keep = iters_txx[fishtrap.index]\n",
      "txy_keep = iters_txy[fishtrap.index]\n",
      "tyy_keep = iters_tyy[fishtrap.index]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make dataframe with posteriors\n",
      "rake_posteriors = pd.concat([txx_keep, tyy_keep, txy_keep, fishtrap], axis=1)\n",
      "\n",
      "rake_posteriors.columns = ['txx', 'tyy', 'txy', 'likelihood']\n",
      "rake_posteriors.to_csv(out_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}