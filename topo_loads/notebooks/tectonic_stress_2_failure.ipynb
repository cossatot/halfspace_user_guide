{
 "metadata": {
  "name": "",
  "signature": "sha256:4e8a6d0322e27c8ff29c761cff463f38329c8a088b9fe345ac7e757fbae76144"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayesian inversion for tectonic stresses\n",
      "## Step 2: Analysis of friction and fluid pressure at failure"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sys\n",
      "import halfspace.projections as hsp\n",
      "import time\n",
      "\n",
      "sys.path.append('../aux_scripts/')\n",
      "import stress_comps_vectorized as scv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rake_poster_file = '../results/baloch_rake_posteriors.csv'\n",
      "out_name = '../results/baloch_fail_posteriors.csv'\n",
      "fault_file = '../test_data/baloch_fault_model.csv'\n",
      "\n",
      "fault_df = pd.read_csv(fault_file, index_col=0)\n",
      "r_priors_df = pd.read_csv(rake_poster_file, index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# some initial constants\n",
      "n_trials = r_priors_df.shape[0]\n",
      "n_points = len(fault_df.index)\n",
      "rho = 2700\n",
      "g = 9.81"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.random.seed(70)\n",
      "\n",
      "r_priors = r_priors_df[['txx', 'tyy', 'txy']].values # make numpy array of priors\n",
      "phi_priors = np.random.random(1e4)\n",
      "phi_priors = phi_priors[r_priors_df.index]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make dataframe\n",
      "search_df_cols = ['iter', 'txx', 'tyy', 'txy', 'mu', 'phi', 'pt_index', \n",
      "                  'depth', 'strike', 'dip', 'slip_m']\n",
      "\n",
      "iter_range = np.float_(r_priors_df.index.values) \n",
      "pt_range = np.arange(n_points, dtype='float')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### make list of priors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iter_range = np.float_(r_priors_df.index.values) \n",
      "pt_range = np.arange(n_points, dtype='float')\n",
      "\n",
      "index_list = [[iter_ind, phi_priors[i], r_priors[i,0], \n",
      "               r_priors[i,1], r_priors[i,2], pi]\n",
      "              for i, iter_ind in enumerate(iter_range) for pi in pt_range]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "index_array = np.array(index_list)\n",
      "del index_list\n",
      "\n",
      "iter_index = np.int_(index_array[:,0].copy() )\n",
      "pt_index = np.int_(index_array[:,5].copy() )\n",
      "phi_prior_array = index_array[:,1]\n",
      "t_prior_array = index_array[:,2:5]\n",
      "del index_array"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_df=pd.DataFrame(index=np.arange(len(iter_index)), \n",
      "                       columns=search_df_cols, dtype=float)\n",
      "\n",
      "search_df['iter'] = iter_index\n",
      "search_df['phi'] = phi_prior_array\n",
      "search_df['pt_index'] = pt_index\n",
      "search_df[['txx', 'tyy', 'txy']] = t_prior_array\n",
      "\n",
      "search_df['mxx'] = 0.\n",
      "search_df['mxy'] = 0.\n",
      "search_df['mxz'] = 0.\n",
      "search_df['myy'] = 0.\n",
      "search_df['myz'] = 0.\n",
      "search_df['mzz'] = 0.\n",
      "\n",
      "# This is a list of the columns in the search_df that we are going to\n",
      "# fill with values from the fault slip dataframe.\n",
      "df_fill_cols = ['depth', 'strike', 'dip', 'slip_m',\n",
      "                'mxx', 'myy', 'mxy', 'mzz', 'mxz', 'myz']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This (`df_copy_cols`) might require a little configuration if the names in the fault dataframe are different than those in this example.  But there needs to be a 1:1 correspondance between the column name and order in `df_fill_cols` and `df_copy_cols`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_copy_cols = ['z', 'strike','dip','slip_m',\n",
      "                'xx_stress', 'yy_stress', 'xy_stress', 'zz_stress',\n",
      "                'xz_stress', 'yz_stress']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_col_array = fault_df[df_copy_cols].values    # copy columns\n",
      "df_reps = np.tile(df_col_array, [n_trials, 1])  # make sequential copies\n",
      "search_df[df_fill_cols] = df_reps               # fill df\n",
      "search_df.depth *= 1000                         # make depth into m (orig km)\n",
      "\n",
      "del df_reps # save some RAM\n",
      "\n",
      "# convert from MPa to Pa\n",
      "search_df[['mxx', 'myy', 'mxy', 'mzz', 'mxz', 'myz']] *= 1e6"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "calc fault stresses"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_df['tau_s'] = scv.strike_shear( strike=search_df.strike,\n",
      "                                      dip=search_df.dip, rho=rho, g=g,\n",
      "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
      "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
      "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
      "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
      "                                      txy=search_df.txy, depth=search_df.depth)\n",
      "\n",
      "search_df['tau_d'] = scv.dip_shear( strike=search_df.strike,\n",
      "                                      dip=search_df.dip, rho=rho, g=g,\n",
      "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
      "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
      "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
      "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
      "                                      txy=search_df.txy, depth=search_df.depth)\n",
      "\n",
      "search_df['sig_n_eff'] = scv.eff_normal_stress( strike=search_df.strike,\n",
      "                                      dip=search_df.dip, rho=rho, g=g,\n",
      "                                      mxx=search_df.mxx, myy=search_df.myy,\n",
      "                                      mxy=search_df.mxy, mzz=search_df.mzz,\n",
      "                                      mxz=search_df.mxz, myz=search_df.myz,\n",
      "                                      txx=search_df.txx, tyy=search_df.tyy,\n",
      "                                      txy=search_df.txy, depth=search_df.depth,\n",
      "                                      phi=search_df.phi)\n",
      "\n",
      "search_df['tau_mag'] = np.sqrt(search_df.tau_s**2 + search_df.tau_d**2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Calculate likelihoods\n",
      "\n",
      "### Calculate misfit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate weighted misfits, start filtering\n",
      "mean_slip = fault_df.slip_m.mean()\n",
      "\n",
      "search_df['weighted_tau_misfit']=search_df.tau_mag *search_df.slip_m /mean_slip"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "iters = search_df.groupby('iter')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Filter $\\mu$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Calculate $\\mu$ at failure for each sample"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu_iter = iters.weighted_tau_misfit.mean() / iters.sig_n_eff.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Filter samples that have $0 \\leq \\mu \\leq 1$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mu_real = mu_iter[(0 <= mu_iter) & (mu_iter <= 1)] # make filter\n",
      "\n",
      "phi_iters = iters.phi.mean()               # get phi for each iteration\n",
      "\n",
      "phi_keep = phi_iters[mu_real.index]        # filter phi samples\n",
      "txx_keep = iters.txx.mean()[mu_real.index] # filter stresses and likelihood\n",
      "tyy_keep = iters.tyy.mean()[mu_real.index]\n",
      "txy_keep = iters.txy.mean()[mu_real.index]\n",
      "likelihood_keep = r_priors_df.loc[mu_real.index, 'likelihood']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Now save filtered posteriors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fail_posteriors = pd.concat([txx_keep, tyy_keep, txy_keep, mu_real, phi_keep,\n",
      "                             likelihood_keep], axis=1)\n",
      "\n",
      "fail_posteriors.columns = ['txx','tyy','txy','mu','phi', 'likelihood']\n",
      "\n",
      "fail_posteriors.to_csv(out_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}